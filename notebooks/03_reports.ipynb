{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reports and Interpretation Tutorial\n",
    "\n",
    "In this tutorial, you'll learn how to:\n",
    "\n",
    "1. Generate and save HTML reports\n",
    "2. Interpret report visualizations\n",
    "3. Extract data from reports\n",
    "4. Share reports with stakeholders\n",
    "5. Use reports for decision-making\n",
    "\n",
    "**Time**: ~15 minutes  \n",
    "**Prerequisites**: Complete Tutorials 01 and 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "\n",
    "from justiceai import audit, FairnessEvaluator\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úì Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating a Comprehensive Report\n",
    "\n",
    "Let's create a realistic scenario and generate a full report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create realistic loan approval dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1500,\n",
    "    n_features=12,\n",
    "    n_informative=10,\n",
    "    n_classes=2,\n",
    "    weights=[0.65, 0.35],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataFrame with meaningful feature names\n",
    "feature_names = [\n",
    "    'income', 'credit_score', 'employment_years', 'debt_ratio',\n",
    "    'savings', 'property_value', 'loan_amount', 'age',\n",
    "    'education_level', 'marital_status', 'dependents', 'job_stability'\n",
    "]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['approved'] = y\n",
    "\n",
    "# Add sensitive attribute with bias\n",
    "gender = np.random.choice(['Male', 'Female'], size=1500, p=[0.53, 0.47])\n",
    "# Introduce historical bias\n",
    "male_mask = gender == 'Male'\n",
    "bias_indices = male_mask & (np.random.random(1500) < 0.15)\n",
    "df.loc[bias_indices, 'approved'] = 1\n",
    "\n",
    "df['gender'] = gender\n",
    "\n",
    "print(f\"Dataset created: {df.shape}\")\n",
    "print(f\"\\nApproval rate by gender:\")\n",
    "print(df.groupby('gender')['approved'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "X = df[feature_names]\n",
    "y = df['approved']\n",
    "gender = df['gender']\n",
    "\n",
    "X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(\n",
    "    X, y, gender, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úì Model trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generating the Report\n",
    "\n",
    "Create a complete fairness report with all metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = audit(\n",
    "    model=model,\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    sensitive_attrs=gender_test,\n",
    "    fairness_threshold=0.05,\n",
    "    output_path='loan_fairness_report.html'\n",
    ")\n",
    "\n",
    "print(\"‚úì Report generated: loan_fairness_report.html\")\n",
    "print(\"\\nOpen this file in your browser to see:\")\n",
    "print(\"  - Overall fairness score\")\n",
    "print(\"  - Metric breakdowns by group\")\n",
    "print(\"  - Interactive visualizations\")\n",
    "print(\"  - Actionable recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Interpreting Report Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Overall Fairness Score\n",
    "\n",
    "The overall score (0-100) summarizes model fairness across all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = report.get_overall_score()\n",
    "\n",
    "print(f\"Overall Fairness Score: {score:.1f}/100\\n\")\n",
    "\n",
    "# Interpretation guide\n",
    "if score >= 90:\n",
    "    print(\"üü¢ EXCELLENT: Model exhibits very good fairness\")\n",
    "    print(\"   Action: Safe for production deployment\")\n",
    "elif score >= 70:\n",
    "    print(\"üü° GOOD: Model has minor fairness issues\")\n",
    "    print(\"   Action: Review violations, consider improvements\")\n",
    "elif score >= 50:\n",
    "    print(\"üü† MODERATE: Model has significant fairness concerns\")\n",
    "    print(\"   Action: Address violations before deployment\")\n",
    "else:\n",
    "    print(\"üî¥ POOR: Model has severe fairness problems\")\n",
    "    print(\"   Action: Major revisions required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Fairness Violations\n",
    "\n",
    "Identify specific metrics that failed the fairness threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of violations\n",
    "issues = report.get_issues()\n",
    "\n",
    "print(f\"Found {len(issues)} fairness violation(s):\\n\")\n",
    "\n",
    "if len(issues) > 0:\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"Issue {i}: {issue['metric']}\")\n",
    "        print(f\"  Message: {issue['message']}\")\n",
    "        print(f\"  Severity: {issue['severity']}\")\n",
    "        print(f\"  Impact: {issue.get('impact', 'N/A')}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚úÖ No violations! Model meets fairness criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Summary Statistics\n",
    "\n",
    "Get detailed metrics and group-level statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = report.get_summary()\n",
    "\n",
    "print(\"Summary Statistics:\\n\")\n",
    "print(f\"Overall Score: {summary['overall_score']:.1f}/100\")\n",
    "print(f\"Passes Fairness: {summary['passes_fairness']}\")\n",
    "print(f\"Number of Violations: {summary['n_violations']}\")\n",
    "print(f\"\\nKey Fairness Metrics:\")\n",
    "print(f\"  Statistical Parity Diff: {summary['statistical_parity_diff']:.4f}\")\n",
    "print(f\"  Disparate Impact Ratio: {summary['disparate_impact_ratio']:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "if abs(summary['statistical_parity_diff']) < 0.05:\n",
    "    print(\"  ‚úì Statistical parity is satisfied\")\n",
    "else:\n",
    "    diff = summary['statistical_parity_diff']\n",
    "    advantaged = 'Males' if diff > 0 else 'Females'\n",
    "    print(f\"  ‚úó {advantaged} receive more favorable outcomes\")\n",
    "\n",
    "if summary['disparate_impact_ratio'] >= 0.80:\n",
    "    print(\"  ‚úì Passes 80% rule (legal standard)\")\n",
    "else:\n",
    "    print(f\"  ‚úó Fails 80% rule ({summary['disparate_impact_ratio']:.2f} < 0.80)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Extracting Data for Further Analysis\n",
    "\n",
    "Export report data in various formats for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary as JSON\n",
    "summary_json = json.dumps(summary, indent=2, default=str)\n",
    "\n",
    "# Save to file\n",
    "with open('fairness_summary.json', 'w') as f:\n",
    "    f.write(summary_json)\n",
    "\n",
    "print(\"‚úì Summary exported to fairness_summary.json\")\n",
    "print(\"\\nSample JSON output:\")\n",
    "print(json.dumps(summary, indent=2, default=str)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fairness metrics DataFrame\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'Metric': 'Overall Score',\n",
    "        'Value': f\"{summary['overall_score']:.1f}\",\n",
    "        'Status': '‚úÖ' if summary['passes_fairness'] else '‚ö†Ô∏è'\n",
    "    },\n",
    "    {\n",
    "        'Metric': 'Statistical Parity Diff',\n",
    "        'Value': f\"{summary['statistical_parity_diff']:.4f}\",\n",
    "        'Status': '‚úÖ' if abs(summary['statistical_parity_diff']) < 0.05 else '‚ö†Ô∏è'\n",
    "    },\n",
    "    {\n",
    "        'Metric': 'Disparate Impact',\n",
    "        'Value': f\"{summary['disparate_impact_ratio']:.4f}\",\n",
    "        'Status': '‚úÖ' if summary['disparate_impact_ratio'] >= 0.80 else '‚ö†Ô∏è'\n",
    "    }\n",
    "])\n",
    "\n",
    "# Save as CSV\n",
    "metrics_df.to_csv('fairness_metrics.csv', index=False)\n",
    "\n",
    "print(\"‚úì Metrics exported to fairness_metrics.csv\")\n",
    "print(\"\\nMetrics table:\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Using Reports for Decision Making\n",
    "\n",
    "Practical workflows for different stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 For Data Scientists: Model Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "}\n",
    "\n",
    "evaluator = FairnessEvaluator(fairness_threshold=0.05)\n",
    "results = []\n",
    "\n",
    "print(\"Comparing models:\\n\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Quick fairness check\n",
    "    metrics = evaluator.quick_check(model, X_test, y_test, gender_test)\n",
    "    \n",
    "    # Accuracy\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    acc = accuracy_score(y_test, model.predict(X_test))\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': f\"{acc:.3f}\",\n",
    "        'Fairness Score': f\"{metrics['overall_score']:.1f}\",\n",
    "        'Passes': '‚úÖ' if metrics['passes_fairness'] else '‚ùå'\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\nüí° Choose the model with best balance of accuracy and fairness!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 For ML Engineers: Production Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating production monitoring\n",
    "def monitor_model_fairness(model, data_batch, threshold=70):\n",
    "    \"\"\"Monitor model fairness on new data batches.\"\"\"\n",
    "    X, y, gender = data_batch\n",
    "    \n",
    "    evaluator = FairnessEvaluator()\n",
    "    metrics = evaluator.quick_check(model, X, y, gender)\n",
    "    \n",
    "    score = metrics['overall_score']\n",
    "    \n",
    "    if score < threshold:\n",
    "        return {\n",
    "            'status': 'ALERT',\n",
    "            'score': score,\n",
    "            'message': f'Fairness score dropped to {score:.1f}',\n",
    "            'action': 'Investigate and potentially retrain model'\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'status': 'OK',\n",
    "            'score': score,\n",
    "            'message': 'Model fairness within acceptable range'\n",
    "        }\n",
    "\n",
    "# Simulate monitoring\n",
    "result = monitor_model_fairness(\n",
    "    model,\n",
    "    (X_test, y_test, gender_test),\n",
    "    threshold=70\n",
    ")\n",
    "\n",
    "print(\"Production Monitoring Result:\")\n",
    "print(f\"Status: {result['status']}\")\n",
    "print(f\"Score: {result['score']:.1f}\")\n",
    "print(f\"Message: {result['message']}\")\n",
    "if 'action' in result:\n",
    "    print(f\"Action: {result['action']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 For Compliance Teams: Audit Trail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create compliance report\n",
    "compliance_report = {\n",
    "    'audit_date': pd.Timestamp.now().isoformat(),\n",
    "    'model_type': 'Random Forest Classifier',\n",
    "    'dataset_size': len(X_test),\n",
    "    'fairness_threshold': 0.05,\n",
    "    'overall_score': summary['overall_score'],\n",
    "    'passes_fairness': summary['passes_fairness'],\n",
    "    'violations': len(issues),\n",
    "    'disparate_impact': summary['disparate_impact_ratio'],\n",
    "    'passes_80_rule': summary['disparate_impact_ratio'] >= 0.80,\n",
    "    'lgpd_compliant': summary['passes_fairness'],\n",
    "    'recommendations': [\n",
    "        issue['message'] for issue in issues\n",
    "    ] if issues else ['No recommendations - model is fair']\n",
    "}\n",
    "\n",
    "# Save compliance report\n",
    "with open('compliance_audit.json', 'w') as f:\n",
    "    json.dump(compliance_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úì Compliance audit saved: compliance_audit.json\")\n",
    "print(\"\\nCompliance Summary:\")\n",
    "print(f\"  Audit Date: {compliance_report['audit_date']}\")\n",
    "print(f\"  Overall Score: {compliance_report['overall_score']:.1f}/100\")\n",
    "print(f\"  Passes Fairness: {compliance_report['passes_fairness']}\")\n",
    "print(f\"  Passes 80% Rule: {compliance_report['passes_80_rule']}\")\n",
    "print(f\"  LGPD Compliant: {compliance_report['lgpd_compliant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Best Practices for Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices Checklist\n",
    "\n",
    "#### ‚úÖ Before Deploying to Production:\n",
    "1. Generate comprehensive fairness report\n",
    "2. Review all fairness violations\n",
    "3. Document mitigation strategies\n",
    "4. Get stakeholder approval\n",
    "5. Save baseline fairness metrics\n",
    "\n",
    "#### ‚úÖ During Production:\n",
    "1. Monitor fairness metrics regularly (weekly/monthly)\n",
    "2. Set up alerts for fairness score drops\n",
    "3. Compare against baseline metrics\n",
    "4. Document any fairness drift\n",
    "5. Retrain when necessary\n",
    "\n",
    "#### ‚úÖ For Stakeholder Communication:\n",
    "1. Use HTML reports for visual presentation\n",
    "2. Focus on overall score and key violations\n",
    "3. Explain metrics in non-technical terms\n",
    "4. Provide actionable recommendations\n",
    "5. Include comparison with industry standards\n",
    "\n",
    "#### ‚úÖ For Compliance:\n",
    "1. Archive all fairness audit reports\n",
    "2. Maintain version history\n",
    "3. Document threshold justifications\n",
    "4. Keep audit trail of changes\n",
    "5. Prepare for regulatory review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "‚úÖ **Generate reports**: Create comprehensive HTML fairness reports  \n",
    "‚úÖ **Interpret scores**: Understand overall fairness scores and thresholds  \n",
    "‚úÖ **Analyze violations**: Identify and address fairness issues  \n",
    "‚úÖ **Export data**: Save metrics in JSON/CSV for further analysis  \n",
    "‚úÖ **Compare models**: Evaluate multiple models for best fairness  \n",
    "‚úÖ **Monitor production**: Set up fairness monitoring pipelines  \n",
    "‚úÖ **Compliance**: Create audit trails for regulatory requirements  \n",
    "‚úÖ **Best practices**: Follow guidelines for responsible AI deployment  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Reports are communication tools** - Make them accessible to all stakeholders\n",
    "2. **Monitor continuously** - Fairness can drift over time\n",
    "3. **Document everything** - Keep audit trails for compliance\n",
    "4. **Balance is key** - Consider fairness alongside other metrics\n",
    "5. **Be proactive** - Address fairness issues before deployment\n",
    "\n",
    "## Congratulations! üéâ\n",
    "\n",
    "You've completed all JusticeAI tutorials. You now know how to:\n",
    "- Perform fairness audits quickly\n",
    "- Understand all fairness metrics in depth\n",
    "- Generate and interpret comprehensive reports\n",
    "- Build responsible ML systems\n",
    "\n",
    "## Resources\n",
    "\n",
    "- üìñ [Documentation](https://justiceai-validation.github.io/JusticeAI/)\n",
    "- üêõ [Report Issues](https://github.com/JusticeAI-Validation/JusticeAI/issues)\n",
    "- üí¨ [Community Discussions](https://github.com/JusticeAI-Validation/JusticeAI/discussions)\n",
    "\n",
    "Happy building fair ML systems! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
