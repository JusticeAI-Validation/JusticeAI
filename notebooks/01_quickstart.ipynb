{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JusticeAI Quick Start Tutorial\n",
    "\n",
    "Welcome to JusticeAI! In this tutorial, you'll learn how to:\n",
    "\n",
    "1. Train a simple ML model\n",
    "2. Evaluate its fairness with one line of code\n",
    "3. Interpret the results\n",
    "4. Generate interactive reports\n",
    "\n",
    "**Time**: ~10 minutes  \n",
    "**Prerequisites**: Basic Python and scikit-learn knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Import JusticeAI\n",
    "from justiceai import audit, FairnessEvaluator\n",
    "\n",
    "print(\"âœ“ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Synthetic Dataset\n",
    "\n",
    "We'll create a synthetic dataset that simulates a loan approval scenario with potential gender bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create features and target\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=8,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    weights=[0.6, 0.4],  # Imbalanced classes\n",
    "    flip_y=0.1,  # Some noise\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create a DataFrame\n",
    "feature_names = [f'feature_{i}' for i in range(10)]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Add sensitive attribute: gender\n",
    "# Introduce bias: positive outcomes more common for one gender\n",
    "gender = np.random.choice(['Male', 'Female'], size=1000, p=[0.55, 0.45])\n",
    "df['gender'] = gender\n",
    "\n",
    "# Introduce correlation between gender and outcome (simulate bias)\n",
    "male_mask = df['gender'] == 'Male'\n",
    "# Make males slightly more likely to get positive outcomes\n",
    "bias_flip = np.random.random(sum(male_mask)) < 0.1\n",
    "df.loc[male_mask, 'target'] = np.where(\n",
    "    bias_flip,\n",
    "    1,\n",
    "    df.loc[male_mask, 'target']\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{df['target'].value_counts()}\")\n",
    "print(f\"\\nGender distribution:\\n{df['gender'].value_counts()}\")\n",
    "print(f\"\\nPositive rate by gender:\")\n",
    "print(df.groupby('gender')['target'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Data and Train Model\n",
    "\n",
    "Split the data and train a Random Forest classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features, target, and sensitive attribute\n",
    "X = df[feature_names]\n",
    "y = df['target']\n",
    "gender = df['gender']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(\n",
    "    X, y, gender, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ“ Model trained!\")\n",
    "print(f\"Accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quick Fairness Audit\n",
    "\n",
    "Now let's evaluate fairness with a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-line fairness audit\n",
    "report = audit(\n",
    "    model=model,\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    sensitive_attrs=gender_test,\n",
    "    fairness_threshold=0.05\n",
    ")\n",
    "\n",
    "print(\"âœ“ Fairness audit complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Interpret Results\n",
    "\n",
    "Let's examine the fairness metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overall fairness score\n",
    "score = report.get_overall_score()\n",
    "print(f\"Overall Fairness Score: {score:.1f}/100\")\n",
    "\n",
    "# Check if model passes fairness threshold\n",
    "passes = report.passes_fairness()\n",
    "print(f\"Passes Fairness Threshold: {passes}\")\n",
    "\n",
    "# Get summary\n",
    "summary = report.get_summary()\n",
    "print(f\"\\nNumber of Violations: {summary['n_violations']}\")\n",
    "print(f\"Statistical Parity Difference: {summary['statistical_parity_diff']:.4f}\")\n",
    "print(f\"Disparate Impact Ratio: {summary['disparate_impact_ratio']:.4f}\")\n",
    "\n",
    "# Interpretation\n",
    "if passes:\n",
    "    print(\"\\nâœ… Model meets fairness criteria!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Model has fairness issues that need attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Examine Fairness Issues\n",
    "\n",
    "If there are fairness violations, let's see what they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed issues\n",
    "issues = report.get_issues()\n",
    "\n",
    "if len(issues) > 0:\n",
    "    print(f\"Found {len(issues)} fairness issue(s):\\n\")\n",
    "    for i, issue in enumerate(issues, 1):\n",
    "        print(f\"{i}. {issue['metric']}\")\n",
    "        print(f\"   {issue['message']}\")\n",
    "        print(f\"   Severity: {issue['severity']}\\n\")\n",
    "else:\n",
    "    print(\"No fairness violations detected! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate HTML Report\n",
    "\n",
    "Create an interactive HTML report with visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HTML report\n",
    "report.save_html('quickstart_fairness_report.html')\n",
    "print(\"âœ“ Report saved as 'quickstart_fairness_report.html'\")\n",
    "print(\"\\nOpen this file in your browser to see interactive visualizations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using FairnessEvaluator for More Control\n",
    "\n",
    "For more advanced use cases, use the `FairnessEvaluator` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluator with custom settings\n",
    "evaluator = FairnessEvaluator(fairness_threshold=0.05)\n",
    "\n",
    "# Evaluate model\n",
    "report2 = evaluator.evaluate(\n",
    "    model=model,\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    sensitive_attrs=gender_test\n",
    ")\n",
    "\n",
    "print(f\"Fairness Score: {report2.get_overall_score():.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Quick Check for Rapid Iteration\n",
    "\n",
    "During model development, use `quick_check()` for fast feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check without generating full report\n",
    "metrics = evaluator.quick_check(\n",
    "    model=model,\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    sensitive_attrs=gender_test\n",
    ")\n",
    "\n",
    "print(\"Quick Check Results:\")\n",
    "print(f\"  Score: {metrics['overall_score']:.1f}\")\n",
    "print(f\"  Passes: {metrics['passes_fairness']}\")\n",
    "print(f\"  Violations: {metrics['n_violations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Working with Pre-computed Predictions\n",
    "\n",
    "If you already have predictions, you can evaluate them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and probabilities\n",
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate predictions directly\n",
    "report3 = evaluator.evaluate_predictions(\n",
    "    y_true=y_test,\n",
    "    y_pred=predictions,\n",
    "    sensitive_attrs=gender_test,\n",
    "    y_pred_proba=probabilities\n",
    ")\n",
    "\n",
    "print(f\"Score from predictions: {report3.get_overall_score():.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "âœ… How to perform a one-line fairness audit with `audit()`  \n",
    "âœ… How to interpret fairness scores and metrics  \n",
    "âœ… How to identify fairness violations  \n",
    "âœ… How to generate interactive HTML reports  \n",
    "âœ… How to use `FairnessEvaluator` for more control  \n",
    "âœ… How to use `quick_check()` for rapid iteration  \n",
    "âœ… How to evaluate pre-computed predictions  \n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 02**: Deep dive into advanced fairness metrics\n",
    "- **Tutorial 03**: Customize reports and visualizations\n",
    "- Check the [documentation](https://justiceai-validation.github.io/JusticeAI/) for more details\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Fairness evaluation is quick**: Just one line of code!\n",
    "2. **Multiple metrics matter**: No single metric tells the whole story\n",
    "3. **Iterate quickly**: Use `quick_check()` during development\n",
    "4. **Report for stakeholders**: Generate beautiful HTML reports for communication\n",
    "\n",
    "Happy fair ML development! ðŸŽ¯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
