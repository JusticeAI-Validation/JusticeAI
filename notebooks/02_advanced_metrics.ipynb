{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Fairness Metrics Tutorial\n",
    "\n",
    "In this tutorial, you'll learn about:\n",
    "\n",
    "1. All fairness metrics in detail\n",
    "2. Pre-training vs post-training metrics\n",
    "3. When to use each metric\n",
    "4. Trade-offs between metrics\n",
    "5. Interpreting complex scenarios\n",
    "\n",
    "**Time**: ~20 minutes  \n",
    "**Prerequisites**: Complete Tutorial 01 (Quick Start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from justiceai import FairnessEvaluator\n",
    "from justiceai.core.metrics.calculator import FairnessCalculator\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Pre-training Metrics\n",
    "\n",
    "Pre-training metrics analyze the dataset **before** model training to identify potential sources of bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Class Balance\n",
    "\n",
    "Checks if the target variable is balanced across sensitive attribute groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset with class imbalance across groups\n",
    "n_samples = 1000\n",
    "\n",
    "# Create two groups with different positive rates\n",
    "group_a_size = 600\n",
    "group_b_size = 400\n",
    "\n",
    "# Group A: 30% positive rate\n",
    "y_a = np.random.choice([0, 1], size=group_a_size, p=[0.7, 0.3])\n",
    "# Group B: 50% positive rate (imbalanced!)\n",
    "y_b = np.random.choice([0, 1], size=group_b_size, p=[0.5, 0.5])\n",
    "\n",
    "y = np.concatenate([y_a, y_b])\n",
    "sensitive_attr = np.array(['A'] * group_a_size + ['B'] * group_b_size)\n",
    "\n",
    "print(\"Class Balance Analysis:\")\n",
    "print(f\"Group A positive rate: {y_a.mean():.3f}\")\n",
    "print(f\"Group B positive rate: {y_b.mean():.3f}\")\n",
    "print(f\"Difference: {abs(y_a.mean() - y_b.mean()):.3f}\")\n",
    "print(\"\\nThis indicates potential bias in the data collection or historical outcomes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Concept Balance (Correlation)\n",
    "\n",
    "Measures if the sensitive attribute correlates with the target outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "sensitive_numeric = (sensitive_attr == 'B').astype(int)\n",
    "correlation = np.corrcoef(sensitive_numeric, y)[0, 1]\n",
    "\n",
    "print(f\"Correlation between sensitive attribute and target: {correlation:.3f}\")\n",
    "print(\"\\nHigh correlation indicates the sensitive attribute\")\n",
    "print(\"is predictive of the outcome, which may lead to unfair models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Post-training Metrics Deep Dive\n",
    "\n",
    "Post-training metrics analyze **model predictions** to measure fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more realistic dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=15,\n",
    "    n_informative=10,\n",
    "    n_redundant=3,\n",
    "    n_classes=2,\n",
    "    weights=[0.6, 0.4],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create sensitive attribute with correlation to outcome\n",
    "gender = np.random.choice(['Male', 'Female'], size=2000, p=[0.52, 0.48])\n",
    "male_mask = gender == 'Male'\n",
    "# Introduce bias: males slightly more likely to be positive class\n",
    "bias_indices = male_mask & (np.random.random(2000) < 0.12)\n",
    "y[bias_indices] = 1\n",
    "\n",
    "# Split and train\n",
    "X_train, X_test, y_train, y_test, gender_train, gender_test = train_test_split(\n",
    "    X, y, gender, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Model trained on {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Statistical Parity (Demographic Parity)\n",
    "\n",
    "**Definition**: P(≈∑=1 | A=Male) should equal P(≈∑=1 | A=Female)\n",
    "\n",
    "**When to use**: When you want equal representation in outcomes (e.g., marketing, university admissions)\n",
    "\n",
    "**Limitation**: Ignores whether predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistical parity manually\n",
    "male_positive_rate = y_pred[gender_test == 'Male'].mean()\n",
    "female_positive_rate = y_pred[gender_test == 'Female'].mean()\n",
    "stat_parity_diff = male_positive_rate - female_positive_rate\n",
    "\n",
    "print(\"Statistical Parity Analysis:\")\n",
    "print(f\"Male positive prediction rate: {male_positive_rate:.3f}\")\n",
    "print(f\"Female positive prediction rate: {female_positive_rate:.3f}\")\n",
    "print(f\"Difference: {stat_parity_diff:.3f}\")\n",
    "print(f\"\\nFair if difference < 0.05: {abs(stat_parity_diff) < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Disparate Impact Ratio (80% Rule)\n",
    "\n",
    "**Definition**: P(≈∑=1 | Unprivileged) / P(≈∑=1 | Privileged) should be ‚â• 0.80\n",
    "\n",
    "**When to use**: Legal compliance (EEOC hiring guidelines), lending decisions\n",
    "\n",
    "**Advantage**: Easy to interpret, legally established threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate disparate impact\n",
    "disparate_impact = female_positive_rate / male_positive_rate\n",
    "\n",
    "print(\"Disparate Impact Analysis (80% Rule):\")\n",
    "print(f\"Ratio: {disparate_impact:.3f}\")\n",
    "print(f\"Passes 80% rule: {disparate_impact >= 0.80}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "if disparate_impact >= 0.80:\n",
    "    print(\"‚úÖ Model meets legal fairness standard\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model may face legal challenges\")\n",
    "    print(f\"   Female rate needs to be at least {male_positive_rate * 0.80:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Equal Opportunity\n",
    "\n",
    "**Definition**: TPR should be equal across groups (among qualified individuals)\n",
    "\n",
    "**When to use**: When false negatives are costly (loan approvals, medical diagnosis)\n",
    "\n",
    "**Focus**: Qualified individuals should have equal chance of positive prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TPR (True Positive Rate) for each group\n",
    "def calculate_tpr(y_true, y_pred, mask):\n",
    "    y_true_masked = y_true[mask]\n",
    "    y_pred_masked = y_pred[mask]\n",
    "    \n",
    "    # TPR = TP / (TP + FN) = TP / all actual positives\n",
    "    positives = y_true_masked == 1\n",
    "    if positives.sum() == 0:\n",
    "        return 0.0\n",
    "    true_positives = (y_pred_masked[positives] == 1).sum()\n",
    "    return true_positives / positives.sum()\n",
    "\n",
    "male_tpr = calculate_tpr(y_test, y_pred, gender_test == 'Male')\n",
    "female_tpr = calculate_tpr(y_test, y_pred, gender_test == 'Female')\n",
    "eq_opp_diff = abs(male_tpr - female_tpr)\n",
    "\n",
    "print(\"Equal Opportunity Analysis:\")\n",
    "print(f\"Male TPR (among qualified males): {male_tpr:.3f}\")\n",
    "print(f\"Female TPR (among qualified females): {female_tpr:.3f}\")\n",
    "print(f\"Difference: {eq_opp_diff:.3f}\")\n",
    "print(f\"\\nFair if difference < 0.05: {eq_opp_diff < 0.05}\")\n",
    "print(\"\\nThis means qualified individuals of both genders\")\n",
    "print(\"have similar chances of being approved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Equalized Odds\n",
    "\n",
    "**Definition**: Both TPR and FPR should be equal across groups\n",
    "\n",
    "**When to use**: When both false positives AND false negatives matter (fraud detection, criminal justice)\n",
    "\n",
    "**Most stringent**: Requires fairness for both qualified and unqualified individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate FPR (False Positive Rate) for each group\n",
    "def calculate_fpr(y_true, y_pred, mask):\n",
    "    y_true_masked = y_true[mask]\n",
    "    y_pred_masked = y_pred[mask]\n",
    "    \n",
    "    # FPR = FP / (FP + TN) = FP / all actual negatives\n",
    "    negatives = y_true_masked == 0\n",
    "    if negatives.sum() == 0:\n",
    "        return 0.0\n",
    "    false_positives = (y_pred_masked[negatives] == 1).sum()\n",
    "    return false_positives / negatives.sum()\n",
    "\n",
    "male_fpr = calculate_fpr(y_test, y_pred, gender_test == 'Male')\n",
    "female_fpr = calculate_fpr(y_test, y_pred, gender_test == 'Female')\n",
    "fpr_diff = abs(male_fpr - female_fpr)\n",
    "\n",
    "# Equalized odds is max of TPR and FPR differences\n",
    "eq_odds = max(eq_opp_diff, fpr_diff)\n",
    "\n",
    "print(\"Equalized Odds Analysis:\")\n",
    "print(f\"\\nTPR difference: {eq_opp_diff:.3f}\")\n",
    "print(f\"FPR difference: {fpr_diff:.3f}\")\n",
    "print(f\"Equalized Odds (max): {eq_odds:.3f}\")\n",
    "print(f\"\\nFair if < 0.05: {eq_odds < 0.05}\")\n",
    "print(\"\\nThis is the most comprehensive fairness metric,\")\n",
    "print(\"ensuring fairness for both groups in all scenarios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using JusticeAI for Comprehensive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let JusticeAI calculate all metrics automatically\n",
    "evaluator = FairnessEvaluator(fairness_threshold=0.05)\n",
    "\n",
    "report = evaluator.evaluate(\n",
    "    model=model,\n",
    "    X=X_test,\n",
    "    y_true=y_test,\n",
    "    sensitive_attrs=gender_test\n",
    ")\n",
    "\n",
    "# Get complete summary\n",
    "summary = report.get_summary()\n",
    "\n",
    "print(\"Complete Fairness Analysis:\")\n",
    "print(f\"Overall Score: {summary['overall_score']:.1f}/100\")\n",
    "print(f\"Passes Fairness: {summary['passes_fairness']}\")\n",
    "print(f\"Violations: {summary['n_violations']}\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  Statistical Parity Diff: {summary['statistical_parity_diff']:.4f}\")\n",
    "print(f\"  Disparate Impact: {summary['disparate_impact_ratio']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Trade-offs Between Metrics\n",
    "\n",
    "Due to the **Fairness Impossibility Theorem**, you cannot satisfy all metrics simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two models with different fairness profiles\n",
    "print(\"Comparing Models with Different Fairness Trade-offs:\\n\")\n",
    "\n",
    "# Model 1: High accuracy, potentially less fair\n",
    "model1 = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# Model 2: Simpler model, potentially more fair\n",
    "model2 = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both\n",
    "report1 = evaluator.evaluate(model1, X_test, y_test, gender_test)\n",
    "report2 = evaluator.evaluate(model2, X_test, y_test, gender_test)\n",
    "\n",
    "# Compare\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc1 = accuracy_score(y_test, model1.predict(X_test))\n",
    "acc2 = accuracy_score(y_test, model2.predict(X_test))\n",
    "\n",
    "print(f\"Random Forest:\")\n",
    "print(f\"  Accuracy: {acc1:.3f}\")\n",
    "print(f\"  Fairness Score: {report1.get_overall_score():.1f}/100\")\n",
    "print(f\"\\nLogistic Regression:\")\n",
    "print(f\"  Accuracy: {acc2:.3f}\")\n",
    "print(f\"  Fairness Score: {report2.get_overall_score():.1f}/100\")\n",
    "\n",
    "print(\"\\nüí° Often there's a trade-off between model complexity and fairness!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Choosing the Right Metric\n",
    "\n",
    "Decision guide for metric selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Selection Guide\n",
    "\n",
    "| Scenario | Recommended Metric | Why |\n",
    "|----------|-------------------|-----|\n",
    "| **Hiring** | Disparate Impact | Legal requirement (80% rule) |\n",
    "| **Loan Approval** | Equal Opportunity | Don't deny qualified applicants |\n",
    "| **Medical Diagnosis** | Equalized Odds | Both false pos/neg are critical |\n",
    "| **Marketing** | Statistical Parity | Equal exposure to opportunities |\n",
    "| **Risk Scoring** | Calibration | Probabilities must be accurate |\n",
    "| **Criminal Justice** | Equalized Odds | High stakes for both types of errors |\n",
    "| **University Admissions** | Equal Opportunity | Focus on qualified candidates |\n",
    "\n",
    "### Key Questions to Ask:\n",
    "\n",
    "1. **What are the consequences of false positives vs false negatives?**\n",
    "   - If FN worse ‚Üí Equal Opportunity\n",
    "   - If both matter ‚Üí Equalized Odds\n",
    "   - If neither worse ‚Üí Statistical Parity\n",
    "\n",
    "2. **Do you need legal compliance?**\n",
    "   - Yes ‚Üí Disparate Impact (80% rule)\n",
    "\n",
    "3. **Are base rates different between groups?**\n",
    "   - Yes ‚Üí Equal Opportunity or Equalized Odds\n",
    "   - No ‚Üí Statistical Parity okay\n",
    "\n",
    "4. **Do you use probability scores?**\n",
    "   - Yes ‚Üí Also check Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "‚úÖ **Pre-training metrics**: Identify data bias before modeling  \n",
    "‚úÖ **Statistical Parity**: Equal outcomes across groups  \n",
    "‚úÖ **Disparate Impact**: Legal fairness standard (80% rule)  \n",
    "‚úÖ **Equal Opportunity**: Fairness for qualified individuals  \n",
    "‚úÖ **Equalized Odds**: Comprehensive fairness metric  \n",
    "‚úÖ **Trade-offs**: Cannot satisfy all metrics simultaneously  \n",
    "‚úÖ **Metric selection**: Choose based on your use case  \n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **No single metric is perfect** - Use multiple metrics\n",
    "2. **Context matters** - Choose metrics based on use case\n",
    "3. **Trade-offs exist** - Balance fairness with other goals\n",
    "4. **Monitor continuously** - Fairness can drift over time\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **Tutorial 03**: Customize reports and interpret visualizations\n",
    "- Explore [API documentation](https://justiceai-validation.github.io/JusticeAI/) for advanced features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
